{"status":"ok","feed":{"url":"https://medium.com/feed/@kiranraghavendra","title":"Stories by Kiran Raghavendra on Medium","link":"https://medium.com/@kiranraghavendra?source=rss-66683ea78d68------2","author":"","description":"Stories by Kiran Raghavendra on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*zAIddcsU9eZh-NJNk2WQXg.jpeg"},"items":[{"title":"Intro to Feature Handling and Sci-kit Learn","pubDate":"2021-03-31 13:01:29","link":"https://medium.com/dscjssstu/intro-to-feature-handling-and-sci-kit-learn-911d11aaf479?source=rss-66683ea78d68------2","guid":"https://medium.com/p/911d11aaf479","author":"Kiran Raghavendra","thumbnail":"","description":"\n<p>In our previous articles, we have discussed in-depth, the math and the intuition behind Linear regression and Logistic regression, i.e the Machine Learning part of an end-to-end project. Today, we will look at some of the concepts involved while handling a real-world dataset i.e the data preprocessing part of it, using the famous sci-kit learn module. Don\u2019t worry, we will go through the entire pipeline involved in a real-world data science project Data Exploration, Data Preprocessing, Data Cleaning, Feature Selection, Model Selection one by\u00a0one.</p>\n<p><strong>Installation:</strong></p>\n<p>Below is the installation link of the sci-kit learn module for Windows, Mac OS, and Linux\u00a0OS.</p>\n<p><a href=\"https://scikit-learn.org/stable/install.html\">Click Here</a></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/160/1*FiRI9nCcHGQSK_GiR-KTQw.png\"><figcaption>Sci-kit Learn (Credits: Sci-kit Learn\u00a0Website)</figcaption></figure><p><strong>Different types of features:</strong></p>\n<p><strong><em>1. Numerical features:</em></strong></p>\n<p>Numerical features can be of two types. Continuous and Discrete. The preprocessing technique commonly used on numerical features is feature scaling, which we\u2019ll discuss in the same\u00a0article.</p>\n<p>1. <em>Continuous features:</em> As in the name, the features follows a continuous distribution. Continuous data is data that can take any value. Height, weight, temperature, and length are all examples of continuous data.</p>\n<p>2. <em>Discrete features: </em>Discrete data is information that can only take certain values. The number of decayed teeth in a child, Birth Year, Year of manufacture are all examples of discrete\u00a0data.</p>\n<p><strong>2. <em>Categorical features:</em></strong></p>\n<p>Categorical features are of two types, Nominal and Ordinal features. The preprocessing technique commonly used on nominal and ordinal features are One hot encoding and Ordinal encoding, which we\u2019ll discuss further in this\u00a0article.</p>\n<p><em>1. Nominal features:</em> Nominal data is made of discrete values with no numerical relationship between the different categories\u200a\u2014\u200amean and median are meaningless. These features are handled using One Hot Encoding. Country of origin, Bike owned are examples of nominal features. Why? Let me elaborate! Country of origin may take on values such as India, England, Australia, etc. Here, each category has no relation with one other i.e India, England, Australia are just different countries with no relation with each other. Still, having trouble understanding? Don\u2019t worry, your doubts will clear up once I explain ordinal features.</p>\n<p><em>2. Ordinal features:</em> As the name suggests, ordinal features have a predefined order between the categories. Ordinal data are discrete integers that can be ranked or sorted. A defining characteristic is that the distance between any two numbers is not known. Quality of food, Education are examples of ordinal features. Let me elaborate. Education can take on values such as High school graduate, Pre uni graduate, PhD graduate, etc. Here, the different categories are related. Suppose the target variable, is the probability of landing a job in the future, A PhD graduate certainly has a higher value than a High school graduate. Similarly, the quality of food can take on values such as Bad, Avg, Good, Excellent. Excellent is certainly a better review than Avg, and this relation cannot be captured by simply One Hot encoding this feature, hence it must be ordinally encoded.</p>\n<p><strong>Feature Scaling:</strong></p>\n<p>This is the data preprocessing technique commonly used on numerical features. Machine learning algorithms, that use gradient descent as an optimization technique (Eg: Linear regression, Logistic regression) require the features to be\u00a0scaled.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/481/1*Qls-pttmUWFb2X5HkM12pw.png\"><figcaption>Gradient Descent formula (Credits: Analytics Vidya)</figcaption></figure><p>If the features are of a different order of magnitude, the steps taken will be of different sizes for different features. Thus, in turn, the gradient descent will not be smooth. To ensure that gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the\u00a0model.</p>\n<p>There are mainly two feature\u00a0scaling:</p>\n<ol>\n<li>Feature standardization</li>\n<li>Feature normalization</li>\n</ol>\n<p><strong><em>Feature Standardization:</em></strong></p>\n<p>Feature standardization, is the process done to convert the current distribution to a SND i.e Standard Normal Distribution with \u03bc=0 and \u03c3=1 where \u03bc is the mean (average) and \u03c3 is the standard deviation from the mean; Standard scores (also called <strong><em>z</em></strong> scores) of the samples are calculated as\u00a0follows:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/361/1*R18P-VM6jtK54MIfxkB4tA.png\"><figcaption>Feature_new_val=(Feature_old_val-Feature_mean)/Feature std deviation</figcaption></figure><p>This can be done, manually by hand or by using the StandardScaler in the sci-kit learn module as follows. You can execute this code snippet on your laptop, or using Google Colab for free and check out the results for yourself.</p>\n<pre>from sklearn import preprocessing<br>import pandas as pd<br>import numpy as np</pre>\n<pre>df=pd.io.parsers.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv',header=None,usecols=[1,2])</pre>\n<pre>df.columns=['Alcohol', 'Malic acid']<br>std_scale = preprocessing.StandardScaler().fit(df[['Alcohol', 'Malic acid']])<br>df_std = std_scale.transform(df[['Alcohol', 'Malic acid']])<br>df_std=pd.DataFrame(df_std)<br>df_std.columns=['Alcohol', 'Malic acid']<br>print('Data frame before feature standardization')<br>print(df.head())<br>print()<br>print('Data frame after feature standardization')<br>print(df_std.head())</pre>\n<pre>print()</pre>\n<pre>print('Data frame after feature standardization')</pre>\n<pre>print(df_std.head())</pre>\n<p><strong><em>Feature Normalization:</em></strong></p>\n<p>This is also called Min-Max scaling. In this approach, the data is scaled to a fixed range - usually 0 to 1. The cost of having this bounded range - in contrast to standardization, is that we will end up with smaller standard deviations, which can suppress the effect of outliers (Outliers are some examples in the data that are very different from the majority of the data and is of no significance, its presence ends up confusing our Machine learning model, hence must be minimized or removed, but more about that in our upcoming articles).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/466/1*_xSuxV8nDAKCjB_Ft3Ewqw.jpeg\"></figure><p>This can be done manually or by using the MinMaxScaler in the sci-kit learn module as follows. You can execute this code snippet on your laptop, or using Google Colab for free and check out the results for yourself.</p>\n<pre>from sklearn import preprocessing<br>import pandas as pd<br>import numpy as np</pre>\n<pre>df=pd.io.parsers.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv',header=None,usecols=[1,2])</pre>\n<pre>df.columns=['Alcohol', 'Malic acid']<br>minmax_scale = preprocessing.MinMaxScaler().fit(df[['Alcohol', 'Malic acid']])<br>df_minmax = minmax_scale.transform(df[['Alcohol', 'Malic acid']])<br>df_minmax=pd.DataFrame(df_minmax)<br>df_minmax.columns=['Alcohol', 'Malic acid']<br>print('Data frame before feature standardization')<br>print(df.head())<br>print()<br>print('Data frame after feature standardization')<br>print(df_minmax.head())</pre>\n<p><strong>One Hot Encoding:</strong></p>\n<p>One-Hot Encoding is the process of creating dummy variables<em>. </em>This preprocessing technique is used to handle categorical features, nominal variables to be specific. This can be done using the get_dummies in the Pandas module or using the One Hot Encoder in the sci-kit learn\u00a0module.</p>\n<pre>from sklearn import preprocessing<br>from sklearn.preprocessing import OneHotEncoder<br>import pandas as pd<br>import numpy as np<br>data={<br>'name':['Smith','Julie','Marie'],<br>'gender':['Male','Female','Female'],<br>'country':['England','Ireland','Japan']<br>}<br>print('Before One Hot encoding')<br>df = pd.DataFrame(data)<br>print(df)<br>print()<br>print('After One Hot encoding')<br>one_hot=OneHotEncoder(sparse=False)<br>df_onehot=df['country']<br>df_onehot=pd.DataFrame(df_onehot)<br>one_hot.fit(df_onehot)<br>onehot_labels=one_hot.transform(df_onehot)<br>onehot_df=pd.DataFrame(onehot_labels)<br>result=pd.concat([df,onehot_df],axis=1)<br>result.drop('country',axis=1,inplace=True)<br>print(result)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/613/1*QaaysRAnU9A-hygB-lpopg.jpeg\"><figcaption>Output for above\u00a0code</figcaption></figure><p><strong>Ordinal Encoding:</strong></p>\n<p>Ordinal Encoding is used to handle ordinal features. This can be done manually by hand or using the Ordinal Encoder provided by sci-kit learn. I prefer ordinal encoding manually, as it gives us more control over the value we want to encode. You can execute this code snippet on your laptop, or using Google Colab for free and check out the results for yourself.</p>\n<pre>from sklearn import preprocessing<br>import pandas as pd<br>import numpy as np<br>data={<br>'name':['Smith','Julie','Marie','John'],<br>'gender':['Male','Female','Female','Male'],<br>'review':['Avg','Good','Excellent','Worst']<br>}<br>print('Before One Hot encoding')<br>df = pd.DataFrame(data)<br>print(df)<br>print()<br>print('After One Hot encoding')<br>review_dict={'Worst':0,'Avg':1,'Good':2,'Excellent':3}<br>df['review_int']=df.review.map(review_dict)<br>print(df)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/665/1*b7pybU7XgT_xFCZtTGxuWQ.jpeg\"><figcaption>Output for above\u00a0code</figcaption></figure><p>In just one article you\u2019ve learned how to handle numerical features, ordinal features, and nominal features. Pat yourself for accomplishing it. We will take up Data Cleaning, Feature Selection, Model selection in the upcoming articles. Stay\u00a0Tuned!</p>\n<h3>Connect \u2022 Learn \u2022\u00a0Grow</h3>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=911d11aaf479\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/dscjssstu/intro-to-feature-handling-and-sci-kit-learn-911d11aaf479\">Intro to Feature Handling and Sci-kit Learn</a> was originally published in <a href=\"https://medium.com/dscjssstu\">GDSCJSSSTU</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<p>In our previous articles, we have discussed in-depth, the math and the intuition behind Linear regression and Logistic regression, i.e the Machine Learning part of an end-to-end project. Today, we will look at some of the concepts involved while handling a real-world dataset i.e the data preprocessing part of it, using the famous sci-kit learn module. Don\u2019t worry, we will go through the entire pipeline involved in a real-world data science project Data Exploration, Data Preprocessing, Data Cleaning, Feature Selection, Model Selection one by\u00a0one.</p>\n<p><strong>Installation:</strong></p>\n<p>Below is the installation link of the sci-kit learn module for Windows, Mac OS, and Linux\u00a0OS.</p>\n<p><a href=\"https://scikit-learn.org/stable/install.html\">Click Here</a></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/160/1*FiRI9nCcHGQSK_GiR-KTQw.png\"><figcaption>Sci-kit Learn (Credits: Sci-kit Learn\u00a0Website)</figcaption></figure><p><strong>Different types of features:</strong></p>\n<p><strong><em>1. Numerical features:</em></strong></p>\n<p>Numerical features can be of two types. Continuous and Discrete. The preprocessing technique commonly used on numerical features is feature scaling, which we\u2019ll discuss in the same\u00a0article.</p>\n<p>1. <em>Continuous features:</em> As in the name, the features follows a continuous distribution. Continuous data is data that can take any value. Height, weight, temperature, and length are all examples of continuous data.</p>\n<p>2. <em>Discrete features: </em>Discrete data is information that can only take certain values. The number of decayed teeth in a child, Birth Year, Year of manufacture are all examples of discrete\u00a0data.</p>\n<p><strong>2. <em>Categorical features:</em></strong></p>\n<p>Categorical features are of two types, Nominal and Ordinal features. The preprocessing technique commonly used on nominal and ordinal features are One hot encoding and Ordinal encoding, which we\u2019ll discuss further in this\u00a0article.</p>\n<p><em>1. Nominal features:</em> Nominal data is made of discrete values with no numerical relationship between the different categories\u200a\u2014\u200amean and median are meaningless. These features are handled using One Hot Encoding. Country of origin, Bike owned are examples of nominal features. Why? Let me elaborate! Country of origin may take on values such as India, England, Australia, etc. Here, each category has no relation with one other i.e India, England, Australia are just different countries with no relation with each other. Still, having trouble understanding? Don\u2019t worry, your doubts will clear up once I explain ordinal features.</p>\n<p><em>2. Ordinal features:</em> As the name suggests, ordinal features have a predefined order between the categories. Ordinal data are discrete integers that can be ranked or sorted. A defining characteristic is that the distance between any two numbers is not known. Quality of food, Education are examples of ordinal features. Let me elaborate. Education can take on values such as High school graduate, Pre uni graduate, PhD graduate, etc. Here, the different categories are related. Suppose the target variable, is the probability of landing a job in the future, A PhD graduate certainly has a higher value than a High school graduate. Similarly, the quality of food can take on values such as Bad, Avg, Good, Excellent. Excellent is certainly a better review than Avg, and this relation cannot be captured by simply One Hot encoding this feature, hence it must be ordinally encoded.</p>\n<p><strong>Feature Scaling:</strong></p>\n<p>This is the data preprocessing technique commonly used on numerical features. Machine learning algorithms, that use gradient descent as an optimization technique (Eg: Linear regression, Logistic regression) require the features to be\u00a0scaled.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/481/1*Qls-pttmUWFb2X5HkM12pw.png\"><figcaption>Gradient Descent formula (Credits: Analytics Vidya)</figcaption></figure><p>If the features are of a different order of magnitude, the steps taken will be of different sizes for different features. Thus, in turn, the gradient descent will not be smooth. To ensure that gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the\u00a0model.</p>\n<p>There are mainly two feature\u00a0scaling:</p>\n<ol>\n<li>Feature standardization</li>\n<li>Feature normalization</li>\n</ol>\n<p><strong><em>Feature Standardization:</em></strong></p>\n<p>Feature standardization, is the process done to convert the current distribution to a SND i.e Standard Normal Distribution with \u03bc=0 and \u03c3=1 where \u03bc is the mean (average) and \u03c3 is the standard deviation from the mean; Standard scores (also called <strong><em>z</em></strong> scores) of the samples are calculated as\u00a0follows:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/361/1*R18P-VM6jtK54MIfxkB4tA.png\"><figcaption>Feature_new_val=(Feature_old_val-Feature_mean)/Feature std deviation</figcaption></figure><p>This can be done, manually by hand or by using the StandardScaler in the sci-kit learn module as follows. You can execute this code snippet on your laptop, or using Google Colab for free and check out the results for yourself.</p>\n<pre>from sklearn import preprocessing<br>import pandas as pd<br>import numpy as np</pre>\n<pre>df=pd.io.parsers.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv',header=None,usecols=[1,2])</pre>\n<pre>df.columns=['Alcohol', 'Malic acid']<br>std_scale = preprocessing.StandardScaler().fit(df[['Alcohol', 'Malic acid']])<br>df_std = std_scale.transform(df[['Alcohol', 'Malic acid']])<br>df_std=pd.DataFrame(df_std)<br>df_std.columns=['Alcohol', 'Malic acid']<br>print('Data frame before feature standardization')<br>print(df.head())<br>print()<br>print('Data frame after feature standardization')<br>print(df_std.head())</pre>\n<pre>print()</pre>\n<pre>print('Data frame after feature standardization')</pre>\n<pre>print(df_std.head())</pre>\n<p><strong><em>Feature Normalization:</em></strong></p>\n<p>This is also called Min-Max scaling. In this approach, the data is scaled to a fixed range - usually 0 to 1. The cost of having this bounded range - in contrast to standardization, is that we will end up with smaller standard deviations, which can suppress the effect of outliers (Outliers are some examples in the data that are very different from the majority of the data and is of no significance, its presence ends up confusing our Machine learning model, hence must be minimized or removed, but more about that in our upcoming articles).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/466/1*_xSuxV8nDAKCjB_Ft3Ewqw.jpeg\"></figure><p>This can be done manually or by using the MinMaxScaler in the sci-kit learn module as follows. You can execute this code snippet on your laptop, or using Google Colab for free and check out the results for yourself.</p>\n<pre>from sklearn import preprocessing<br>import pandas as pd<br>import numpy as np</pre>\n<pre>df=pd.io.parsers.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv',header=None,usecols=[1,2])</pre>\n<pre>df.columns=['Alcohol', 'Malic acid']<br>minmax_scale = preprocessing.MinMaxScaler().fit(df[['Alcohol', 'Malic acid']])<br>df_minmax = minmax_scale.transform(df[['Alcohol', 'Malic acid']])<br>df_minmax=pd.DataFrame(df_minmax)<br>df_minmax.columns=['Alcohol', 'Malic acid']<br>print('Data frame before feature standardization')<br>print(df.head())<br>print()<br>print('Data frame after feature standardization')<br>print(df_minmax.head())</pre>\n<p><strong>One Hot Encoding:</strong></p>\n<p>One-Hot Encoding is the process of creating dummy variables<em>. </em>This preprocessing technique is used to handle categorical features, nominal variables to be specific. This can be done using the get_dummies in the Pandas module or using the One Hot Encoder in the sci-kit learn\u00a0module.</p>\n<pre>from sklearn import preprocessing<br>from sklearn.preprocessing import OneHotEncoder<br>import pandas as pd<br>import numpy as np<br>data={<br>'name':['Smith','Julie','Marie'],<br>'gender':['Male','Female','Female'],<br>'country':['England','Ireland','Japan']<br>}<br>print('Before One Hot encoding')<br>df = pd.DataFrame(data)<br>print(df)<br>print()<br>print('After One Hot encoding')<br>one_hot=OneHotEncoder(sparse=False)<br>df_onehot=df['country']<br>df_onehot=pd.DataFrame(df_onehot)<br>one_hot.fit(df_onehot)<br>onehot_labels=one_hot.transform(df_onehot)<br>onehot_df=pd.DataFrame(onehot_labels)<br>result=pd.concat([df,onehot_df],axis=1)<br>result.drop('country',axis=1,inplace=True)<br>print(result)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/613/1*QaaysRAnU9A-hygB-lpopg.jpeg\"><figcaption>Output for above\u00a0code</figcaption></figure><p><strong>Ordinal Encoding:</strong></p>\n<p>Ordinal Encoding is used to handle ordinal features. This can be done manually by hand or using the Ordinal Encoder provided by sci-kit learn. I prefer ordinal encoding manually, as it gives us more control over the value we want to encode. You can execute this code snippet on your laptop, or using Google Colab for free and check out the results for yourself.</p>\n<pre>from sklearn import preprocessing<br>import pandas as pd<br>import numpy as np<br>data={<br>'name':['Smith','Julie','Marie','John'],<br>'gender':['Male','Female','Female','Male'],<br>'review':['Avg','Good','Excellent','Worst']<br>}<br>print('Before One Hot encoding')<br>df = pd.DataFrame(data)<br>print(df)<br>print()<br>print('After One Hot encoding')<br>review_dict={'Worst':0,'Avg':1,'Good':2,'Excellent':3}<br>df['review_int']=df.review.map(review_dict)<br>print(df)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/665/1*b7pybU7XgT_xFCZtTGxuWQ.jpeg\"><figcaption>Output for above\u00a0code</figcaption></figure><p>In just one article you\u2019ve learned how to handle numerical features, ordinal features, and nominal features. Pat yourself for accomplishing it. We will take up Data Cleaning, Feature Selection, Model selection in the upcoming articles. Stay\u00a0Tuned!</p>\n<h3>Connect \u2022 Learn \u2022\u00a0Grow</h3>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=911d11aaf479\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/dscjssstu/intro-to-feature-handling-and-sci-kit-learn-911d11aaf479\">Intro to Feature Handling and Sci-kit Learn</a> was originally published in <a href=\"https://medium.com/dscjssstu\">GDSCJSSSTU</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["feature-handling","numerical-features","scikit-learn","ml-unravelled","categorical-feature"]},{"title":"Intro to Machine Learning- A Smart Digest","pubDate":"2020-11-10 04:11:11","link":"https://medium.com/dscjssstu/intro-to-machine-learning-a-smart-digest-65d0ff421f14?source=rss-66683ea78d68------2","guid":"https://medium.com/p/65d0ff421f14","author":"Kiran Raghavendra","thumbnail":"","description":"\n<h4>Why Machine Learning?</h4>\n<p>Machine learning is all that you hear nowadays. Everyone is interested in it and here\u2019s why you should be too.<br>It is, what most advanced technologies around us use. For example, Face recognition on your phone, Speech Recognition, Recommendation Systems on Netflix, Self-driving cars, Credit Card Fraud Detection, Cancer Detection, Game training, etc. Its applications are boundless and\u00a0endless.</p>\n<p><strong>So is it the same as Artificial Intelligence?</strong><br>Artificial Intelligence is vast and comprises of many realms such as Natural Language Processing, Computer Vision, Robotics, Expert Systems, etc.<br>Machine Learning is another such realm under Artificial Intelligence.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*VRbrA-5P0bs5qNseo-eU-A.jpeg\"></figure><h4>What is Machine learning?</h4>\n<p>To make the machine learn something new over time and in turn, make it more competent in taking better decisions/making accurate predictions on its\u00a0own.</p>\n<p>Let us consider a car/bot moving on a road filled with setbacks. Does the code below, uses machine learning?</p>\n<pre>if obstacle seen:<br>    if obstacle on right:<br>         Move left<br>    if obstacle on left:<br>         Move right<br>    if obstacle ahead:<br>        Stop and move left/right<br>if no obstacle:<br>    continue straight</pre>\n<p><strong>No</strong>, It\u2019s not!<br>First of all, the car/bot is neither learning anything nor doing anything new. It\u2019s just following the given instructions like a clown! We\u2019d be in a lot of trouble if we made Self-Driving Cars in this way. Now, you might ask what\u2019s wrong with the above code? There are a million unforeseen circumstances that can happen in the real world, the road where the car is driving. We cannot possibly code for all the million virtues that can\u00a0occur.</p>\n<p>For example: Let\u2019s say there are obstacles on the left, right, and ahead of the car, it would first go to the right, then left, and then stops when it sees the obstacle in front of it, leading to all kinds of chaos and mishaps. Now, this is where machine learning comes in. When unexpected situations arise, it is an expectation that a car takes an accurate and safe decision on its\u00a0own.</p>\n<h4>Machine Learning: The Classification</h4>\n<p>Every system using machine learning is trained on large amounts of data before it can make accurate predictions. The various ways of training, according to necessary real-world scenarios, leads to the below classification.</p>\n<ol>\n<li>\n<strong>Supervised Learning:<br></strong>Before making a prediction, the system gets trained on data and while training, the correct predictions/decisions are given to the machine so that it can rectify it even if it makes a mistake, thus by not making that mistake in the future. <br>Here for every input <strong>X</strong> taken by the model, it gives some output <strong>Y</strong>. <br><strong>A</strong> <strong>real-world example</strong>, A kid mistakenly perceives a bus as a truck. The teacher corrects him so that the kid learns what makes a bus, a BUS. The kid learns some traits of the bus, for example, many window panes, a large number of seats, etc. which will help him recognize it correctly in the future.<br>Some examples of supervised learning are <em>Regression, Classification, Object Detection</em>, etc. If you\u2019re not familiar with the terms, don\u2019t worry. We\u2019ll go through it one by\u00a0one.</li>\n<li>\n<strong>Unsupervised Learning:</strong><br><strong> </strong>In unsupervised learning, there is no such aspect of correct or wrong predictions/decisions while training. The model is allowed to make perception of the given data, in its way. It discovers patterns and information about the input that was not detected before.<br>Here for every input<strong> X</strong> taken by the model, it <em>doesn\u2019t produce</em> some output <strong>Y</strong>.<br><strong>A real-world example</strong>, Suppose a kid is given a few toys and asked to separate them. He ends up separating them as\u00a0shown.</li>\n</ol>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*rS8xvkcB9jj3hjZMBu__XA.jpeg\"><figcaption>Different groups made by the kid on this\u00a0own</figcaption></figure><p>Here there is no right and wrong. Both the classifications made by the kid is correct. In the <strong>first classification</strong>, he classified them based on <em>colour</em>, and in the <strong>second classification</strong>, he grouped the toys based on the <em>number of wheels</em>. <br>Some examples of unsupervised learning are <em>Clustering, Recommender Systems, Dimensionality Reduction</em>, etc. No worries if you\u2019re now to these\u00a0terms.</p>\n<p><strong>3. Reinforcement Learning:<br></strong>Here the model is trained using rewards and punishments. In simple terms, if the model takes a correct decision, it\u2019s rewarded but, if it takes a wrong decision, then it\u2019s punished.<br><strong>A real-world example</strong>, Suppose we are training a dog to perform a task say a handshake. If the dog does the handshake, it gets <em>five treats</em> as a reward.<br>If he doesn\u2019t do the handshake, let\u2019s NOT punish our poor dog and give <em>one treat</em> instead of <em>five</em>. Hence, after many trials, our dog learns that it gets rewarded for every correct task it does. The same follows for the machine learning model too.<br><strong>Example</strong>: Training a machine learning agent to play a\u00a0game.</p>\n<p>Here\u2019s a fun game for you to play, called <a href=\"https://quickdraw.withgoogle.com/\"><strong><em>Quick Draw</em></strong></a>. It showcases one of the remarkable capabilities of Machine Learning.<br>If you feel all this was just theoretical, don\u2019t worry we\u2019ll get our hands dirty with some python code and go in-depth into the ML algorithms in our upcoming articles. <br>Stay\u00a0tuned.</p>\n<h4>Connect \u2022 Learn \u2022\u00a0Grow</h4>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=65d0ff421f14\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/dscjssstu/intro-to-machine-learning-a-smart-digest-65d0ff421f14\">Intro to Machine Learning- A Smart Digest</a> was originally published in <a href=\"https://medium.com/dscjssstu\">GDSCJSSSTU</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<h4>Why Machine Learning?</h4>\n<p>Machine learning is all that you hear nowadays. Everyone is interested in it and here\u2019s why you should be too.<br>It is, what most advanced technologies around us use. For example, Face recognition on your phone, Speech Recognition, Recommendation Systems on Netflix, Self-driving cars, Credit Card Fraud Detection, Cancer Detection, Game training, etc. Its applications are boundless and\u00a0endless.</p>\n<p><strong>So is it the same as Artificial Intelligence?</strong><br>Artificial Intelligence is vast and comprises of many realms such as Natural Language Processing, Computer Vision, Robotics, Expert Systems, etc.<br>Machine Learning is another such realm under Artificial Intelligence.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*VRbrA-5P0bs5qNseo-eU-A.jpeg\"></figure><h4>What is Machine learning?</h4>\n<p>To make the machine learn something new over time and in turn, make it more competent in taking better decisions/making accurate predictions on its\u00a0own.</p>\n<p>Let us consider a car/bot moving on a road filled with setbacks. Does the code below, uses machine learning?</p>\n<pre>if obstacle seen:<br>    if obstacle on right:<br>         Move left<br>    if obstacle on left:<br>         Move right<br>    if obstacle ahead:<br>        Stop and move left/right<br>if no obstacle:<br>    continue straight</pre>\n<p><strong>No</strong>, It\u2019s not!<br>First of all, the car/bot is neither learning anything nor doing anything new. It\u2019s just following the given instructions like a clown! We\u2019d be in a lot of trouble if we made Self-Driving Cars in this way. Now, you might ask what\u2019s wrong with the above code? There are a million unforeseen circumstances that can happen in the real world, the road where the car is driving. We cannot possibly code for all the million virtues that can\u00a0occur.</p>\n<p>For example: Let\u2019s say there are obstacles on the left, right, and ahead of the car, it would first go to the right, then left, and then stops when it sees the obstacle in front of it, leading to all kinds of chaos and mishaps. Now, this is where machine learning comes in. When unexpected situations arise, it is an expectation that a car takes an accurate and safe decision on its\u00a0own.</p>\n<h4>Machine Learning: The Classification</h4>\n<p>Every system using machine learning is trained on large amounts of data before it can make accurate predictions. The various ways of training, according to necessary real-world scenarios, leads to the below classification.</p>\n<ol>\n<li>\n<strong>Supervised Learning:<br></strong>Before making a prediction, the system gets trained on data and while training, the correct predictions/decisions are given to the machine so that it can rectify it even if it makes a mistake, thus by not making that mistake in the future. <br>Here for every input <strong>X</strong> taken by the model, it gives some output <strong>Y</strong>. <br><strong>A</strong> <strong>real-world example</strong>, A kid mistakenly perceives a bus as a truck. The teacher corrects him so that the kid learns what makes a bus, a BUS. The kid learns some traits of the bus, for example, many window panes, a large number of seats, etc. which will help him recognize it correctly in the future.<br>Some examples of supervised learning are <em>Regression, Classification, Object Detection</em>, etc. If you\u2019re not familiar with the terms, don\u2019t worry. We\u2019ll go through it one by\u00a0one.</li>\n<li>\n<strong>Unsupervised Learning:</strong><br><strong> </strong>In unsupervised learning, there is no such aspect of correct or wrong predictions/decisions while training. The model is allowed to make perception of the given data, in its way. It discovers patterns and information about the input that was not detected before.<br>Here for every input<strong> X</strong> taken by the model, it <em>doesn\u2019t produce</em> some output <strong>Y</strong>.<br><strong>A real-world example</strong>, Suppose a kid is given a few toys and asked to separate them. He ends up separating them as\u00a0shown.</li>\n</ol>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*rS8xvkcB9jj3hjZMBu__XA.jpeg\"><figcaption>Different groups made by the kid on this\u00a0own</figcaption></figure><p>Here there is no right and wrong. Both the classifications made by the kid is correct. In the <strong>first classification</strong>, he classified them based on <em>colour</em>, and in the <strong>second classification</strong>, he grouped the toys based on the <em>number of wheels</em>. <br>Some examples of unsupervised learning are <em>Clustering, Recommender Systems, Dimensionality Reduction</em>, etc. No worries if you\u2019re now to these\u00a0terms.</p>\n<p><strong>3. Reinforcement Learning:<br></strong>Here the model is trained using rewards and punishments. In simple terms, if the model takes a correct decision, it\u2019s rewarded but, if it takes a wrong decision, then it\u2019s punished.<br><strong>A real-world example</strong>, Suppose we are training a dog to perform a task say a handshake. If the dog does the handshake, it gets <em>five treats</em> as a reward.<br>If he doesn\u2019t do the handshake, let\u2019s NOT punish our poor dog and give <em>one treat</em> instead of <em>five</em>. Hence, after many trials, our dog learns that it gets rewarded for every correct task it does. The same follows for the machine learning model too.<br><strong>Example</strong>: Training a machine learning agent to play a\u00a0game.</p>\n<p>Here\u2019s a fun game for you to play, called <a href=\"https://quickdraw.withgoogle.com/\"><strong><em>Quick Draw</em></strong></a>. It showcases one of the remarkable capabilities of Machine Learning.<br>If you feel all this was just theoretical, don\u2019t worry we\u2019ll get our hands dirty with some python code and go in-depth into the ML algorithms in our upcoming articles. <br>Stay\u00a0tuned.</p>\n<h4>Connect \u2022 Learn \u2022\u00a0Grow</h4>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=65d0ff421f14\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/dscjssstu/intro-to-machine-learning-a-smart-digest-65d0ff421f14\">Intro to Machine Learning- A Smart Digest</a> was originally published in <a href=\"https://medium.com/dscjssstu\">GDSCJSSSTU</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["machine-learning","unsupervised-learning","ml-unravelled","supervised-learning","reinforcement-learning"]}]}